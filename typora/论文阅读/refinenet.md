### refinenet

**介绍：**

这里的密集预测任务是为给定图像中的每个像素分配一个类标签或连续值。

空间池化和卷积的多个阶段通常会在每个维度上减少32倍的最终输出预测，从而失去许多更精细的图像结构。解决这一限制的一种方法是学习反卷积滤波器作为上采样操作，以生成高分辨率特征图。反卷积运算无法恢复卷积前向降采样运算后丢失的底层视觉特征。

高级语义特征有助于图像区域的类别识别，而低级视觉特征有助于为高分辨率预测生成清晰、详细的边界。

我们的主要贡献如下:

1)我们提出了一个多路径细化网络(RefineNet)，它利用多个抽象层次的特征进行高分辨率密集预测。RefineNet以递归的方式用细粒度的低级特征细化低分辨率(粗)特征，以生成高分辨率的特征映射。我们的模型是灵活的，因为它可以以各种方式进行级联和修改。

2)我们的级联refinenet可以有效地端到端训练，这对于良好的预测性能至关重要。更具体地说，RefineNet中的所有组件都使用带有身份映射的残差连接，这样梯度可以通过短程和远程残差连接直接传播，从而实现有效和高效的端到端训练。

3)我们提出了一种新的网络组件，我们称之为“链式剩余池”，它能够从大图像区域捕获背景上下文。它通过有效地池化具有多个窗口大小的特征，并通过残差连接和可学习权值将它们融合在一起来实现这一目标。

![image-20230927165524582](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230927165524582.png)

对于我们的标准多路径架构，我们根据特征图的分辨率将预训练的ResNet(使用ImageNet训练)划分为4个块，并采用具有4个RefineNet单元的4级联架构，每个单元直接连接到一个ResNet块的输出以及级联中的前一个RefineNet块。

我们将RefineNet-m表示为连接到ResNet中block-m输出的RefineNet块。在实践中，每个ResNet输出通过一个卷积层来适应维度。**尽管所有的refinenet共享相同的内部架构，但它们的参数并不是绑定的**，从而允许更灵活地适应各个细节级别。按照图2(c)中自下而上的说明，我们从ResNet中的最后一个块开始，并将ResNet块4的输出连接到RefineNet-4。在这里，RefineNet-4只有一个输入，而RefineNet-4作为一组额外的卷积，使预训练的ResNet权重适应手头的任务。在下一阶段，RefineNet-4的输出和ResNet block-3作为两路输入馈送到RefineNet-3。RefineNet-3的目标是使用来自**ResNet block-3的高分辨率特征**来细化**RefineNet-4在上一阶段输出的低分辨率特征图**。类似地，RefineNet-2和RefineNet-1通过融合来自后一层的高级信息和来自前一层的高分辨率但较低的特征来重复这种分阶段的细化。最后一步，**将最终的高分辨率特征图馈送到一个密集的soft-max层**，**以密集分数图的形式进行最终的预测**。然后使用双线性插值对该分数图进行上采样以匹配原始图像。

用高分辨率细化低分辨率也就是用低级特征去补足高级特征。

**作用**：我们在ResNet和RefineNet模块中的块之间引入了**远程剩余连接**。在前向传递过程中，这些远程残差连接传递编码视觉细节的低级特征，用于精炼粗糙的高级特征映射。在训练步骤中，远程残差连接允许梯度直接传播到早期卷积层，有助于有效地进行端到端训练。

![image-20230927165531814](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230927165531814.png)

**残差卷积单元**。每个RefineNet块的第一部分由一个自适应卷积集组成，主要用于为我们的任务微调预训练的ResNet权重。每个输入路径依次通过两个残差卷积单元(RCU)， RCU是原始ResNet中卷积单元的简化版本，其中去除了批处理归一化层(参见图3(b))。

**多分辨率融合**。然后通过多分辨率融合块将所有路径输入融合成高分辨率特征图，如图3(c)所示。该块首先应用卷积进行输入自适应，生成相同特征维度(输入中最小的一个)的特征映射，**然后对所有(较小的)特征映射进行上采样，使其达到输入的最大分辨率**。最后，对所有特征映射进行求和融合。如果只有一条输入路径(如图2(c)中的RefineNet-4)，则输入路径将直接穿过该区块，不做任何改变。

**链式残差池化。**如图3(d)所示。所提出的链式残差池化方法旨在从大图像区域捕获背景内容。

它能够有效地汇集具有多个窗口大小的特征，并使用可学习的权重将它们融合在一起。特别是，该组件被构建为多个池化块的链，每个池化块由一个最大池化层和一个卷积层组成。一个池化块将前一个池化块的输出作为输入。因此，当前池化块能够重用先前池化操作的结果，从而在不使用大池化窗口的情况下从大区域访问特征。在我们的实验中，我们报告的最佳结果是基于在一个链式剩余池化模块中设置4个池化块。在一个池化块中，每**个池化操作之后都有一个卷积，作为求和融合的加权层。**

图4显示了链式剩余池的另一种架构。通过交换卷积层和池化层在一个池化中的位置，对图3(d)所示的架构进行了修改。**在经过池化层之前，这个卷积层将学习适应输入特征并适应输入特征的重要性**。在我们的观察中，与原始体系结构相比，这种替代体系结构有时在某些数据集中的性能可能略好一些。

![image-20230927194506985](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230927194506985.png)

**输出块**：每个RefineNet块的最后一步是另一个残差卷积单元(RCU)。这导致在每个块之间有三个rcu序列。为了在最后一个RefineNet-1块中反映这种行为，我们在最后一个softmax预测步骤之前放置了两个额外的rcu。这里的目标是在多路径融合特征映射上使用非线性操作来生成用于进一步处理或最终预测的特征。经过该块后，特征维度保持不变。

**非线性变化**

使用恒等映射的残差连接允许梯度直接从一个块传播到任何其他块，这个概念鼓励为快捷连接保持干净的信息路径，这样这些连接就不会被任何非线性层或组件“阻塞”。相反，非线性操作被放置在主要信息路径的分支上。我们遵循这一原则来开发RefineNet中的各个组件，包括所有卷积单元。

正是这种特殊的策略使得多级联的RefineNet能够得到有效的训练。注意，我们在链式剩余池块中包含了一个非线性激活层(ReLU)。我们观察到，这个ReLU对于后续池化操作的有效性很重要，它也使模型对学习率的变化不那么敏感。我们观察到，每个RefineNet区块中一个单独的ReLU不会显著降低梯度流的有效性。



**注释**：它们能够提供较为抽象的理解和分类，但缺少细节和具体的特征描述。例如，在图像识别中，粗的高级语义特征可能表示物体的整体类别（如“狗”、“汽车”），而不考虑具体的细节特征。

细粒度的低级特征则更关注数据或概念的细节和具体特征，通常是基于底层数据或特征的原始表示。这些特征可以是像素级别的图像特征或文本中的个别单词或短语。细粒度的低级特征更加具体和详细，可以提供更多关于数据的细节信息，但可能缺乏整体的抽象和语义。

**远程剩余连接**的原理是，将前一层的输出与当前层的输入相加，这样可以跨越多个层级传播信息，并且有效地减轻了网络的训练难度。通过远程剩余连接，网络可以更容易地学习到残差，并在训练过程中避免梯度消失或梯度爆炸问题。

**RCU**的原理是通过递归级联的方式，从大尺寸到小尺寸，逐渐缩小感受野以获取更多细节信息。在RCU中，每一级都由若干个相同结构的子网络组成，每个子网络都具有接受来自上一级的信息和产生新特征的能力。

**RESnet**的原理是通过引入残差块和跳跃连接来改善模型的训练和优化过程。每个残差块由两个卷积层组成，其中使用了批量归一化和激活函数。跳跃连接允许信息直接跨越块级别传播，并保留了之前层级的良好特征。

残**差连接背后的思想是基于残差学习的概念**。在深度学习中，网络的目标是学习输入与目标输出之间的映射关系。而残差学习则通过学习残差函数，即输入与目标输出之间的差异，来实现这一目标。

具体地说，**残差连接将网络的输入直接与输出相加，并将它们作为网络的输出。这样做的目的是引入残差信息，使网络更容易对输入进行适当的调整，并学习到输入与输出之间的有效映射**。通过残差连接，网络可以学习到差异部分（残差），将其添加到输入上以更好地逼近目标输出。

**恒等映射规则**是指在残差连接中，如果**两个输入尺寸和输出尺寸相同，那么这个残差连接可以被视为恒等映射**。这意味着网络可以通过直接复制输入信号并将其添加到输出中来完全保留输入的特征。这种恒等映射规则帮助网络更好地利用残差连接，以提高模型的优化效果。

**短程剩余连接**是指在一个RCU或残差池组件中本地的短暂连接，而**远程剩余连接**是指RefineNet模块和ResNet块之间的连接。

**密集分数图**（Dense Score Map）是指表示图像中每个像素点所属类别或某个属性的得分图。密集分数图常用于图像分割、姿态估计、关键点检测等任务中。对于图像分割任务，密集分数图可以用来确定每个像素点属于哪个目标类别。对于姿态估计或关键点检测任务，密集分数图可以用来表示图像中每个像素点属于关键点的置信度得分。

**低分辨率**特征指的是在图像或视频处理过程中，将图像或视频的分辨率降低，从而得到的较**粗糙**的**高级**特征表示。这样做的目的通常是为了减少计算量和存储空间，同时尽可能地保留主要的视觉信息。

**细粒度**的**低级**特征通常是指在图像或视频处理中较低层次的特征表达，比如边缘、纹理、颜色等。这些特征具有**较高的空间分辨率**，可以提供更加详细和精确的信息。

**上采样操作**通常用于将低分辨率的特征映射**增加**到与高分辨率特征映射相同的尺寸。这可以帮助提高网络对小尺寸结构和细节的感知能力。值得注意的是，上采样操作可能会导致一些信息的丢失或模糊处理。

**池化**（Pooling）是深度学习中常用的一种操作，其作用是对特征图进行降采样，减少参数数量，并增加模型的平移不变性和鲁棒性。以下是池化在深度学习中的作用和原理：

作用：

1. 降采样：池化操作通过**降低**特征图的空间维度，减少了后续层的计算量和参数数量，提高了模型的运算效率。
2. 特征提取：池化操作可以通过聚合局部信息来提取特征。例如，最大池化会选择最显著的特征值，平均池化则会将周围特征的平均值作为输出，从而提取出图像中的重要特征。
3. 平移不变性：池化操作在对特征图进行降采样的同时，粗化了特征图的空间表示，使其对平移变化不敏感。这使得网络能够更好地理解和处理图像中的平移不变的特征，提高了模型的鲁棒性。

原理：
池化操作通常在卷积层之后进行。其原理可以分为以下步骤：

1. 定义池化窗口大小：池化操作会定义一个窗口大小，通常是一个正方形或矩形区域。
2. 滑动窗口：池化窗口以固定的步幅在特征图上滑动，每次取窗口内的特征进行池化操作。
3. 池化操作：根据池化方式的不同，可以是最大池化、平均池化等。最大池化会选择窗口内的最大值作为输出，平均池化会计算窗口内特征的平均值作为输出。
4. 输出特征图：**通过池化操作得到的输出特征值将替代原始特征图中对应窗口区域的值**，从而得到降采样后的特征图。