### 泛化综述

#### 介绍

传统的机器学习模型是基于训练和测试数据相同且独立分布的i.i.d假设进行训练的。这种假设往往不存在。

领域泛化的目标是从一个或几个不同但相关的领域(即不同的训练数据集)中学习一个模型，该模型将在未知的测试领域上很好地泛化。

比如由绘画、素描和卡通领域泛化到摄像领域。

#### 背景

#####  Formalization of Domain Generalization

![image-20230826165310210](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230826165310210.png)

![image-20230826165328353](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230826165328353.png)

gaol: achieve a minimum prediction error on an unseen test domain Stest 

##### 相关研究领域

**多任务学习**旨在通过同时学习多个任务来提高模型的泛化能力，而域泛化则旨在通过学习不同域之间的共性来提高模型的泛化能力。具体来说，多任务学习通常涉及到同时学习多个相关的任务，例如语音识别和语言模型等。这些任务通常具有共性和相互依赖性，并且可以共享一些底层特征和知识。

**迁移学习**在源任务上训练模型，旨在提高模型在不同但相关的目标领域/任务上的性能。预训练微调是迁移学习中常用的策略，在迁移学习中，源域和目标域具有不同的任务，目标域在训练中被访问。在DG中，目标域无法访问，训练和测试任务通常是相同的，但它们具有不同的分布。

**DA（Domain adaptation）**和DG的不同之处在于，DA可以访问目标域数据，而DG在训练过程中无法看到这些数据。

**元学习**：机器学习是先人为调参，之后直接训练特定任务下深度模型。元学习则是先通过其它的任务训练出一个较好的超参数，然后再对特定任务进行训练。

元学习会去学习所有需要由人去设置和定义的参数变量 。在这里参数变量 属于集合为 ，则有：

![c342e9f2511ac2084bbf47c6eef960c6.png](https://img-blog.csdnimg.cn/img_convert/c342e9f2511ac2084bbf47c6eef960c6.png)

不同的元学习，就要去学集合中不同的元素，相应的就会有不同的研究领域。

**终身学习**，即持续学习，关注的是多个顺序域/任务之间的学习能力。它要求模型通过适应新知识，同时保留以前学到的经验，不断地学习。这也与DG不同，因为它可以在每个时间步骤中访问目标域，并且它不显式地处理跨域的不同分布。

**Zero-shot learning**旨在从已知类中学习模型，并对训练中未见类别的样本进行分类。而领域泛化一般研究的是训练数据和测试数据来自同一类但分布不同的问题。

##### 理论

pass

##### 方法论

**数据操作**:这类方法侧重于操作输入以帮助学习一般表示。沿着这条线，有两种流行的技术:（a)数据增强，主要基于输入数据的增强、随机化和转换;（b).数据生成，生成不同的样本以帮助泛化

典型的增强操作包括翻转、旋转、缩放、裁剪、添加噪声等。它们已被广泛应用于监督学习中，通过减少过拟合来提高模型的泛化性能。M(·)可以实例化为这些数据增强函数。

随机化通常是通过基于有限的训练样本生成可以模拟复杂环境的新数据来完成的，M(·)函数被实现为几个手动转换(通常用于图像数据)，例如:改变物体的位置和纹理，改变物体的数量和形状，修改照明和相机视图，以及向数据中添加不同类型的随机噪声。

对抗性数据增强具有一定的优化目标，可供网络使用。然而，其优化过程往往涉及对抗性训练，因此是困难的。

**表示学习**:这类方法在领域泛化中是最流行的。有两种代表性的技术:a).域不变表示学习，它执行核，对抗性训练，域之间显式特征对齐，或学习域不变表示的不变风险最小化;b). Feature disentanglement，它试图将特征解纠缠成域共享或特定于域的部分，以便更好地进行泛化

Kernel-based machine learning的基本思想是将低维空间中线性不可分的数据映射到高维空间中，使得数据在高维空间中变得线性可分。这种方法有助于解决一些机器学习任务中的非线性问题，如分类、聚类、回归等。

对抗性训练训练生成器和鉴别器。鉴别器被训练来区分领域，而生成器被训练来欺骗鉴别器来学习域不变特征表示。

特征分布对齐旨在通过特定方法来使不同样本点的特征分布相同或相似。其目标是让所有样本都共享相同的特征分布，从而提高机器学习算法的性能。

IRM的核心思想是在模型训练过程中强制要求模型对混淆变量不敏感，即使混淆变量在不同的域中有所改变，模型也能够保持稳定的预测能力。

基于解纠缠的DG方法通常将特征表示分解为可理解的组合/子特征，其中一个特征是domain-shared/invariant feature，另一个是与 domain-specific feature。基于解纠缠的DG主要可分为三类：多分量分析、生成建模和因果激励方法。在**多分量分析**中，通常使用领域共享和领域特定的网络参数来提取领域共享和领域特定的特征。**生成模型**可以从数据生成过程的角度进行解纠缠。这种方法试图从领域级、样本级和标签级三个层次来制定样本的生成机制。**因果关系**给出了系统在干预下如何行为的信息，因此它自然适用于迁移学习任务，因为域移位可以被视为一种干预。

**学习策略**:这类方法侧重于利用一般的学习策略来提升泛化能力，主要包括以下几种方法:（a)集成学习，依靠集成的力量来学习统一的、泛化的预测函数;（b)元学习，基于“学习到学习”机制，通过构建元学习任务模拟领域转移来学习一般知识;（c）梯度运算，通过直接对梯度进行操作来学习广义表示;（d).分布鲁棒优化，学习训练域的最坏分布情况;（e).自监督学习，构建借口任务来学习广义表征

集成学习通过使用特定的网络架构设计和训练策略来利用多个源领域之间的关系来提高泛化。他们假设任何一个样本都可以看作是多个源域的一个集成样本，因此整体的预测结果可以看作是多个域网络的叠加。

元学习的关键思想是通过基于优化的方法、基于度量的学习或基于模型的方法从多个任务中学习一个通用模型。

使用梯度信息来迫使网络学习广义表示。

分布式鲁棒优化(distributed robust optimization, DRO)的目标是学习最坏分布情况下的模型，希望它能很好地泛化到测试数据。

自监督学习是一种通用的范例，可以应用于任何现有的DG方法，特别是在训练域中没有标签的无监督DG[79]。基于ssl的DG的另一个可能的应用是多域数据的预训练，它可以训练强大的预训练模型，同时还可以处理域转移。

##### 其他领域泛化研究领域

![image-20230827135442562](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230827135442562.png)

在Def. 2中设置M = 1，得到**单源DG**。与传统DG (M > 1)相比，由于训练域的多样性较低，单源DG更具挑战性。因此，该问题的关键是利用数据生成技术生成新的域，以增加训练数据的多样性和信息量。

半监督DG不需要训练域的完整标签。通常使用现有的**半监督学习算法**，如FixMatch和FlexMatch来学习未标记样本的伪标签。

在**federated DG**中，模型不访问原始训练数据;相反，它们聚合来自不同客户机的参数。在这种情况下，通过泛化设计更好的聚合方案是关键

Open DG共享类似的通用域适应设置，其中训练和测试标签空间是不同的。无监督DG假设训练域中的所有标签都是不可访问的。

##### 应用

![image-20230827135152874](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230827135152874.png)

##### 数据集、评估和基准

pass