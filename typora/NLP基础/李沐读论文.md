## 李沐读论文

### Transformer论文逐段精读【论文精读】

[Transformer论文逐段精读【论文精读】_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.999.0.0&vd_source=bee013e22d10b6d2e417a16b33fbc3f5)

[TOC]

#### Attention Is All You Need

##### Abstract

我们提出了一种新的简单网络架构，即Transformer，它完全基于注意力机制，完全省去了递归和卷积。可以并行，并且训练时间大大减少。

##### Conclusion

1.应用：在翻译领域应用不错。

2.目标：我们对基于注意力的模型的未来感到兴奋，并计划将其应用于其他任务。使用到图像、音频和视频等领域。

Making generation less sequential is another research goals of ours.

ps：最好把代码放在介绍的最后一行，方便别人直观了解，因为论文部分很难把细节描绘清楚。

#####  Introduction

总体上是简介的补充。

RNN的时序性计算导致难以并行，纯注意力的运算就打破了这种顺序计算的限制。

##### Background（相关工作）

在此部分中，你要表达清楚相关论文的作用、与你的联系以及和你的论点的区别。

1.用多头注意力来模拟CNN的多通道。

2.自我注意，有时称为内部注意，是一种将单个序列的不同位置联系起来以计算序列的表示的注意机制。

##### Model Architecture（重点）

![image-20230706094424661](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230706094424661.png)

1.解码器输入的outputs就是之前的一些输出。

2.Encoder: （1）为了便于这些残差连接，模型中的所有子层以及嵌入层都产生维度d<sub>model</sub>=512的输出。

（2）一般可调整的参数就是N和d<sub>model</sub>

（3）LayerNorm(x + Sublayer(x)),

LayerNorm层归一化的公式：

![image-20230706100700289](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230706100700289.png)

LayerNorm和BatchNorm的区别：

![image-20230706100751283](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230706100751283.png)

蓝色是B，黄色是L。

![image-20230706100827315](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230706100827315.png)

LayerNorm以样本为单位计算，方差和均值都是样本内部计算，所以比较稳定。

BatchNorm是计算全局的方差和样本，因此样本的长短不一影响了稳定性。

Decoder: 多了个带掩码的多注意力层，防止编码器预测时看见整个输入。

3.注意力机制

（1）Scaled Dot-Product Attention

![image-20230706163901811](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230706163901811.png)

![image-20230706165936193](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230706165936193.png)

输出被计算为值的加权和，所以输出的维度和value的维度相同。权重就是Q和K的相似度，不同的注意力机制，相似度的计算不同。

![image-20230706101837233](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230706101837233.png)

Q与K更相近，那么他在求和的时候，占比更重。

为什么选择这个注意力机制？

加性注意力：加性注意力通过将查询向量和键向量的线性组合进行softmax归一化来计算每个元素的权重。

![image-20230706165158885](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230706165158885.png)

*Q*表示查询向量，*K*表示键向量，*V*表示值向量，W<sub>Q</sub>和W<sub>K</sub>分别表示查询和键向量的线性变换矩阵，d<sub>k</sub>表示键向量的维度。

点乘注意力：点乘注意力计算查询向量和键向量之间的点积，并进行缩放。缩放是为了防止softmax的值接近1或者0。

Mask

t时刻,Q<sub>t</sub>可以和K<sub>1</sub>、...、k<sub>t</sub>、...、K<sub>n</sub>计算，但是在计算输出时，只需要t时刻之前的结果。于是将不需要的值变成极小的数，经过softmax后，其值为0。

（2）Multi-Head Attention

使用原因：Scaled Dot-Product Attention没有什么可学习的参数。

Multi-Head Attention将Q、K、V投影到低维空间，投影时的W可以学习，由此可以识别多种模式。通常给与h次机会，将学习好的W返回来计算。

原理：

![image-20230706172605216](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230706172605216.png)

因为最后会concat，并且输出的维度要保持一致。所以d<sub>k</sub> = d<sub>v</sub> = d<sub>model</sub>=h.

（3）Applications of Attention in our Model

![image-20230706094424661](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230706094424661.png)

第1个注意力层：输入经过嵌入层转变为n个长度为d的向量，复制为Q、K、V，即Q==K==V。V与QK计算出来的所有权重求和。

第2个注意力层：输入性质同第一层（自注意力），但是带mask。

第3个注意力层：输入来自于编码器输出的K和V，以及解码器上一层注意力层输出的Q。此层的作用就是将编码器的输出的目标部分选出来。

4.Position-wise Feed-Forward Networks

![image-20230707045256217](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230707045256217.png)

其本质是将MLP用于最后输出，MLP通常指多层感知器（Multilayer Perceptron），是一种前馈神经网络，常用于分类和回归问题。

transform中和RNN中MLP的区别？

![image-20230707045907634](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230707045907634.png)

在transform中，MLP将向量提取的信息进行全局转换。

在RNN中，上一时刻的输出和输入并入到MLP中，完成信息的传递。

5.Embeddings and Softmax

在嵌入层中，我们将输入序列中的每个单词索引映射到对应的向量表示。为了缩放这些向量表示的大小，我们将权重矩阵中的每个元素都乘以一个标量因子p，其中p通常等于1/sqrt(dmodel)。

6.Positional Encoding

句子有时序限制，但注意力机制处理不了，于是在输入时通过位置编码引入时序信息。防止一句话打乱顺序得到相同结果的情况。

![image-20230707051540791](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230707051540791.png)

思路：用维度为512的向量来表示词，同时使用512维的向量表示位置的数字，其具体内容由公式计算。

7.Why Self-Attention

![image-20230707052415741](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230707052415741.png)

三个指标：计算复杂度、时序度（并行能力），即要等前面多少步完成，才能做这一步、路径长度（越短越好）。

#####  Training

1.Training Data and Batching

WMT 2014 English-German dataset consisting of about 4.5 million
sentence pairs

WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary

2.Hardware and Schedule

machine ： 8 NVIDIA P100 GPUs. F

基础模型，每个训练步骤大约需要0.4秒。我们对基础模型进行了总共100000步或12小时的训练。对于我们的大模型（如表3的底线所述），步长为1.0秒。大型模型接受了300000步（3.5天）的训练。

3.Optimizer

Adam optimizer [20] with β1 = 0:9, β2 = 0:98 and  = 10−9。

4.正则化

（1）Residual Dropout ：Pdrop = 0:1.

（2）Label Smoothing：softmax中，常以0表示错误，1表示正确。但是要值极大的时候才能逼近1。为了避免这种情况，常把正确的标签调整为0.9。

we employed label smoothing of value ls = 0.1，虽然模型学习更加不确定，但提高了准确性和BLEU分数。

5.超参数

![image-20230707060303715](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230707060303715.png)

