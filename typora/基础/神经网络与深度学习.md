

第一章

1.1数据

（1） 数据预处理：对数据的原始形式进行初步的数据清理（比如去掉一些 有缺失特征的样本，或去掉一些冗余的数据特征等）和加工（对数值特征进行缩 放和归一化等），并构建成可用于训练机器学习模型的数据集． 

（2） 特征提取：从数据的原始特征中提取一些对特定机器学习任务有用的 高质量特征．比如在图像分类中提取边缘、尺度不变特征变换（Scale Invariant Feature Transform，SIFT）特征，在文本分类中去除停用词等． （3） 特征转换：对特征进行进一步的加工，比如降维和升维． 很多特征转换方法也 都是机器学习方法． 降维包括特征 抽取（Feature Extraction）和特征选择（Feature Selection）两种途径．

1.2表示学习

如果有一种算法可以自动地学 习出有效的特征，并提高最终机器学习模型的性能，那么这种学习就可以叫作表 示学习（Representation Learning）．

好的表示：更多信息、更高层的语义信息、一般性

局部表示

一种表示颜色的方法是以不同名字来命名不同的颜色，这种表示方式叫 作局部表示，也称为离散表示或符号表示．局部表示通常可以表示为one-hot 向 量的形式．

优点：好总结、计算效率高。

缺点：不好扩展、没有关联。

分布式表示

另一种表示颜色的方法是用RGB值来表示颜色，不同颜色对应到R、G、B三 维空间中一个点，这种表示方式叫作分布式表示． 将分 布 式 表 示叫 作分 散式表示可能更容易 理解，即一种颜色的语 义分散到语义空间中 的不同基向量上． 分布式表示通常可以表示为低 维的稠密向量．

![image-20230606101834593](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230606101834593.png)

1.3 深度学习

![image-20230606102148860](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230606102148860.png)

和“浅层学习”不同，深度学习需要解决 的关键问题是贡献度分配问题。

1.4 人工神经网络

人工神经网络与生物神经元类似，由多个节 点（人工神经元）互相连接而成，可以用来对数据之间的复杂关系进行建模．不 同节点之间的连接被赋予了不同的权重，每个权重代表了一个节点对另一个节 点的影响大小．每个节点代表一种特定函数，来自其他节点的信息经过其相应的权重综合计算，输入到一个激活函数中并得到一个新的活性值（兴奋或抑制）． 从系统观点看，人工神经元网络是由大量神经元通过极其丰富和完善的连接而 构成的自适应非线性动态系统．

1.5 本书的知识体系

![image-20230606102738638](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230606102738638.png)

实践

张量

张量是矩阵的扩展与延伸，可以认为是高阶的矩阵。1阶张量为向量，2阶张量为矩阵。张量中元素的类型可以是布尔型数据、整数、浮点数或者复数，但同一张量中所有元素的数据类型均相同。

![img](https://ai-studio-static-online.cdn.bcebos.com/fcdd500471b842a4811bd7ab3f724ab4b9226fc94bf446818904e59ce1bb6e00)

指定数据创建张量

需要注意的是，张量在任何一个维度上的元素数量必须相等。

```
# 创建多维Tensor
ndim_3_Tensor = ([[[1, 2, 3, 4, 5],
                 [6, 7, 8, 9, 10]],
                 [[11, 12, 13, 14, 15],
                 [16, 17, 18, 19, 20]]])
print(ndim_3_Tensor)
```

指定形状创建

```
m, n = 2, 3

# 使用paddle.zeros创建数据全为0，形状为[m, n]的Tensor
zeros_Tensor = paddle.zeros([m, n])

# 使用paddle.ones创建数据全为1，形状为[m, n]的Tensor
ones_Tensor = paddle.ones([m, n])

# 使用paddle.full创建数据全为指定值，形状为[m, n]的Tensor，这里我们指定数据为10
full_Tensor = paddle.full([m, n], 10)
```

指定区间创建

如果要在指定区间内创建张量，可以使用`paddle.arange`、`paddle.linspace`等API。

```
# 使用paddle.arange创建以步长step均匀分隔数值区间[start, end)的一维Tensor
arange_Tensor = paddle.arange(start=1, end=5, step=1)

# 使用paddle.linspace创建以元素个数num均匀分隔数值区间[start, stop]的Tensor
linspace_Tensor = paddle.linspace(start=1, stop=5, num=5)

print('arange Tensor: ', arange_Tensor)
print('linspace Tensor: ', linspace_Tensor)
```

torch

1.使用 `torch.arange(start, end, step)` 函数创建一个以 `start` 开始，以 `end` 结束（不包含 `end`），以 `step` 为步长的张量。

2.使用 `torch.linspace(start, end, steps)` 函数创建一个以 `start` 开始，以 `end` 结束，分成 `steps` 份的张量。

1.2.2 张量的属性

1.2.2.1 张量的形状

张量具有如下形状属性：

- `Tensor.ndim`：张量的维度，例如向量的维度为1，矩阵的维度为2。
- `Tensor.shape`： 张量每个维度上元素的数量。
- `Tensor.shape[n]`：张量第*n*维的大小。第n维也称为轴（axis）。
- `Tensor.size`：张量中全部元素的个数。

![img](https://ai-studio-static-online.cdn.bcebos.com/d8461ef0994549a98c1b253f2a31fe60edb7bb200b964b43a9c25818f22a31c6)

形状的改变

```
reshape_Tensor = paddle.reshape(ndim_3_Tensor, [2, 5, 3])
```

使用reshape时存在一些技巧，比如：

- -1表示这个维度的值是从张量的元素总数和剩余维度推断出来的。因此，有且只有一个维度可以被设置为-1。
- 0表示实际的维数是从张量的对应维数中复制出来的，因此shape中0所对应的索引值不能超过张量的总维度。

还可以通过`paddle.unsqueeze`将张量中的一个或多个维度中插入尺寸为1的维度。

```
ones_Tensor = paddle.ones([5, 10])
new_Tensor1 = paddle.unsqueeze(ones_Tensor, axis=0)
print('new Tensor 1 shape: ', new_Tensor1.shape)
new_Tensor2 = paddle.unsqueeze(ones_Tensor, axis=[1, 2])
print('new Tensor 2 shape: ', new_Tensor2.shape)
```

张量的数据类型

1.飞桨中可以通过`Tensor.dtype`来查看张量的数据类型。

2.通过Numpy数组创建的张量，则与其原来的数据类型保持相同。通过`paddle.to_tensor()`函数可以将Numpy数组转化为张量。

```
import torch
import numpy as np

# 将一个numpy数组转换为torch张量
numpy_array = np.array([1, 2, 3])
tensor = torch.tensor(numpy_array)

# 将一个Python列表转换为torch张量
python_list = [4, 5, 6]
tensor = torch.tensor(python_list)

# 将一个numpy数组转换为torch张量，并指定数据类型
numpy_array = np.array([1.0, 2.0, 3.0])
tensor = torch.tensor(numpy_array, dtype=torch.float32)

# 将一个torch张量转换为指定设备上的张量
tensor = tensor.to(device='cuda')  # 如果有可用的GPU设备，就将张量转移到GPU上
```

如果想改变张量的数据类型，可以通过调用`paddle.cast`API来实现。

```
# paddle.cast可以将输入数据的数据类型转换为指定的dtype并输出。支持输出和输入数据类型相同。
int64_Tensor = paddle.cast(float32_Tensor, dtype='int64')
```

也可以通过`Tensor.numpy()`函数将张量转化为Numpy数组。

```
ndim_1_Tensor = paddle.to_tensor([1., 2.])
# 将当前 Tensor 转化为 numpy.ndarray
print('Tensor to convert: ', ndim_1_Tensor.numpy())
```

张量的访问

索引和切片

我们可以通过索引或切片方便地访问或修改张量。飞桨使用标准的Python索引规则与Numpy索引规则，具有以下特点：

- 基于0−*n*的下标进行索引，如果下标为负数，则从尾部开始计算。
- 通过冒号“:”分隔切片参数start:stop:step来进行切片操作，也就是访问start到stop范围内的部分元素并生成一个新的序列。其中start为切片的起始位置，stop为切片的截止位置，step是切片的步长，这三个参数均可缺省。

针对二维及以上维度的张量，在多个维度上进行索引或切片。索引或切片的第一个值对应第0维，第二个值对应第1维，以此类推，如果某个维度上未指定索引，则默认为“:”。

```
# 定义1个二维Tensor
ndim_2_Tensor = ([[0, 1, 2, 3],
                 [4, 5, 6, 7],
                 [8, 9, 10, 11]])
print("Origin Tensor:", ndim_2_Tensor)
print("First row:", ndim_2_Tensor[0])
print("First row:", ndim_2_Tensor[0, :])
print("First column:", ndim_2_Tensor[:, 0])
print("Last column:", ndim_2_Tensor[:, -1])
print("All element:", ndim_2_Tensor[:])
print("First row and second column:", ndim_2_Tensor[0, 1]
```

算子

算子定义

```
class Op(object):
    def __init__(self):
        super(Op, self).__init__()

    def __call__(self, inputs):
        return self.forward(inputs)

    # 前向函数
    # 输入：张量inputs
    # 输出：张量outputs
    def forward(self, inputs):
        # return outputs

    # 反向函数
    # 输入：最终输出对outputs的梯度outputs_grads
    # 输出：最终输出对inputs的梯度inputs_grads
    def backward(self, outputs_grads):
        # return inputs_grads
```

自动微分

需要注意的是，在进行自动微分时，只有张量的 `requires_grad` 属性被设置为 `True`，才会对其进行梯度计算。同时，只有标量才能进行反向传播，因此 `my_function` 函数的输出必须是一个标量。

```
import torch

# 定义一个张量
x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)

# 定义一个函数
def my_function(x):
    y = x * 2
    z = y.sum()
    return z

# 计算函数的输出
output = my_function(x)

# 计算梯度
output.backward()

# 输出梯度
print(x.grad)
```

第2章 机器学习概述

如何寻找这个“最优”的函数 𝑓 ∗ (𝒙) 是机器学习的关键，一般需要通过学习 算法（Learning Algorithm）𝒜 来完成．  这个寻找过程通常称为学习（Learning） 或训练（Training）过程．

![image-20230607161131640](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230607161131640.png)

机器学习的三个基本要素

模型

对于一个机器学习任务，首先要确定其输入空间𝒳 和输出空间𝒴．不同机器 学习任务的主要区别在于输出空间不同．在二分类问题中𝒴 = {+1, −1}，在𝐶 分 类问题中𝒴 = {1, 2, ⋯ , 𝐶}，而在回归问题中𝒴 = ℝ．

![image-20230607161833881](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230607161833881.png)

 学习准则

模型𝑓(𝒙; 𝜃)的好坏可以通过期望风险（Expected Risk）ℛ(𝜃)来衡量，其定 义为 期 望 风 险也 经 常 称 为期望错误（Expected Error）． 

ℛ(𝜃) = 𝔼(𝒙,𝑦)∼𝑝𝑟 (𝒙,𝑦)[ℒ(𝑦, 𝑓(𝒙; 𝜃))],                                                          (2.11) 

其中𝑝𝑟 (𝒙, 𝑦)为真实的数据分布，ℒ(𝑦, 𝑓(𝒙; 𝜃))为损失函数，用来量化两个变量之 间的差异．

 损失函数

![image-20230607162545108](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230607162545108.png)

![image-20230607162629858](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230607162629858.png)

交叉熵损失函数

交叉熵损失函数（Cross-Entropy Loss Function）一般用于 分类问题．满足概率分布的要求。

![image-20230608100406682](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230608100406682.png)

![image-20230608100515974](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230608100515974.png)

 风险最小化准则

![image-20230608100814173](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230608100814173.png)

结构风险最小化（Structure Risk Minimization，SRM）准则

![image-20230608101020900](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230608101020900.png)

优化算法

参数与超参数 

在机器学习中，优化又可以分为参数优化和超参数优化．模型 𝑓(𝒙; 𝜃)中的𝜃 称为模型的参数，可以通过优化算法进行学习．除了可学习的参数 𝜃 之外，还有一类参数是用来定义模型结构或优化策略的，这类参数叫作超参数 （Hyper-Parameter）．

 梯度下降法

凸函数是指一个实值函数，其图像上任意两点连线组成的线段都在这两点的函数曲线上方。

![image-20230608102005606](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230608102005606.png)

 提前停止

在每次迭代时，把新得到的模型 𝑓(𝒙; 𝜃) 在验证集上进行测试，并计算错误率． 如果在验证集上的错误率不再下降，就停止迭代．这种策略叫提前停止（Early Stop）．如果没有验证集，可以在训练集上划分出一个小比例的子集作为验证集．

 随机梯度下降法

批量梯度下降法相当于是从 真实数据分布中采集 𝑁 个样本，并由它们计算出来的经验风险的梯度来近似期 望风险的梯度．为了减少每次迭代的计算复杂度，我们也可以在每次迭代时只 采集一个样本，计算这个样本损失函数的梯度并更新参数，即随机梯度下降法

小批量梯度下降法

![image-20230608103909693](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230608103909693.png)

机器学习的简单示例——线性回归

 参数学习

我们介绍四种不同的参数估计方法：经验风险最小化、结构风险最小化、最 大似然估计、最大后验估计．

经验风险最小化![image-20230608111206358](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230608111206358.png)

![image-20230608111234403](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230608111234403.png)

 结构风险最小化

![image-20230608111450793](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230608111450793.png)

 最大似然估计

线性回归还可以从建模条件概率𝑝(𝑦|𝒙)的角度来进行参数估计．

![image-20230608113236655](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230608113236655.png)

 偏差-方差分解

一般来说，当一个模型在训练集上的错误率比较高时，说明模型的 拟合能力不够，偏差比较高．这种情况可以通过增加数据特征、提高模型复杂度、 减小正则化系数等操作来改进．当模型在训练集上的错误率比较低，但验证集上 的错误率比较高时，说明模型过拟合，方差比较高．这种情况可以通过降低模型 复杂度、加大正则化系数、引入先验等方法来缓解．此外，还有一种有效降低方差 的方法为集成模型，即通过多个高方差模型的平均来降低方差．

 机器学习算法的类型

![image-20230608115231819](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230608115231819.png)

实践

![image-20230608115641589](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230608115641589.png)

0.数据

数据预处理可分为两个环节：先对收集到的数据进行基本的预处理，如基本的统计、特征归一化和异常值处理等；再将数据划分为训练集、验证集（开发集）和测试集。

- **训练集**：用于模型训练时调整模型的参数，在这份数据集上的误差被称为训练误差；
- **验证集（开发集）**：对于复杂的模型，常常有一些超参数需要调节，因此需要尝试多种超参数的组合来分别训练多个模型，然后对比它们在验证集上的表现，选择一组相对最好的超参数，最后才使用这组参数下训练的模型在测试集上评估测试误差。
- **测试集**：模型在这份数据集上的误差被称为测试误差。训练模型的目的是为了通过从训练数据中找到规律来预测未知数据，因此测试误差是更能反映出模型表现的指标。

1. 模型：实现输入到输出的映射，通常为可学习的函数；
2. 学习准则：模型优化的目标，通常为损失函数和正则化项的加权组合；
3. 优化算法：根据学习准则优化机器学习模型的参数；
4. 评价指标：用来评价学习到的机器学习模型的性能．

线性模型

二分类

二分类（Binary Classification）问题的类别标签 𝑦 只有两种取值，通常可 以设为 {+1, −1} 或 {0, 1}．

![image-20230609105235437](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230609105235437.png)

 多分类

![image-20230609105917291](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230609105917291.png)

（1）“一对其余”方式：把多分类问题转换为 𝐶 个“一对其余”的二分类问 题．这种方式共需要 𝐶 个判别函数，其中第 𝑐 个判别函数 𝑓𝑐 是将类别 𝑐 的样本和 不属于类别𝑐的样本分开． 

（2）“一对一”方式：把多分类问题转换为 𝐶(𝐶 − 1)/2 个“一对一”的二分 类问题．这种方式共需要𝐶(𝐶 − 1)/2个判别函数，其中第(𝑖, 𝑗)个判别函数是把类 别𝑖 和类别𝑗 的样本分开．

（3）“argmax”方式：如果存在一个类别𝑐，相对于所有的其他类别 ̃𝑐( ̃𝑐 ≠ 𝑐)有𝑓𝑐 (𝒙; 𝒘𝑐 ) > 𝑓𝑐̃ (𝒙, 𝒘𝑐̃ )，那么𝒙属于类别𝑐．

Logistic回归

 参数学习

Logistic 回归采用交叉熵作为损失函数，并使用梯度下降法来对参数进行 优化．

![image-20230609124533151](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230609124533151.png)

![image-20230609124540200](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230609124540200.png)

![image-20230609124546410](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230609124546410.png)

Softmax回归

Softmax 回归（Softmax Regression），也称为多项（Multinomial）或多类 （Multi-Class）的Logistic回归，是Logistic回归在多分类问题上的推广。

![image-20230609125950229](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230609125950229.png)

![image-20230609130021930](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230609130021930.png)

 感知器

分类准则（2分类）

![image-20230615094825808](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230615094825808.png)

 参数学习

![image-20230615095006707](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230615095006707.png)

![image-20230615095020772](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230615095020772.png)

分类准则（多分类）

![image-20230615095553813](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230615095553813.png)

广义感知器模型一般用来处理结构化学习问题．当用广义感知器模型来处 理𝐶 分类问题时，𝒚 ∈ {0, 1}𝐶 为类别的one-hot向量表示．

支持向量机

决策



![image-20230615100605083](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230615100605083.png)

支持向量机的决策函数只依赖于𝜆 ∗ 𝑛 > 0的样本点，即支持向量．即 离决策平面距离最近的点．找到一个距离两个类别最近的样本点最远的超平面。

 核函数

支持向量机还有一个重要的优点是可以使用核函数（Kernel Function）隐 式地将样本从原始特征空间映射到更高维的空间，并解决原始特征空间中的线 性不可分问题．

总结和深入阅读

在 Logistic 回归和 Softmax 回归 中，𝒚为类别的one-hot向量表示；在感知器和支持向量机中，𝑦为{+1, −1}．

![image-20230615104431506](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230615104431506.png)

前馈神经网络

激活函数

 Sigmoid型函数

![image-20230615220911717](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230615220911717.png)

![image-20230615221149151](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230615221149151.png)

![image-20230615221241838](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230615221241838.png)

 ReLU函数

![image-20230615222020378](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230615222020378.png)

在训 练时，如果参数在一次不恰当的更新后，第一个隐藏层中的某个 ReLU 神经元在 所有的训练数据上都不能被激活，那么这个神经元自身参数的梯度永远都会是 0，在以后的训练过程中永远不能被激活．这种现象称为死亡 ReLU 问题.

 ELU函数

![image-20230615225242161](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230615225242161.png)

 Softplus函数

![image-20230615225329588](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230615225329588.png)

![image-20230615225412084](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230615225412084.png)

 Swish函数

Swish函数可以看作线性函数和ReLU函数之间的非线性插值函数，其程度由参数𝛽 控制．

![image-20230616104244958](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230616104244958.png)

![image-20230616103959565](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230616103959565.png)

 GELU函数

![image-20230616104318536](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230616104318536.png)

Maxout单元

 Sigmoid 型 函数、ReLU 等激活函数的输入是神经元的净输入 𝑧，是一个标量．而 Maxout 单 元的输入是上一层神经元的全部原始输出，是一个向量𝒙 = [𝑥1 ; 𝑥2 ; ⋯ ; 𝑥𝐷]．

![image-20230616104843930](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230616104843930.png)

网络结构

 前馈网络

前馈网络中各个神经元按接收信息的先后分为不同的组．每一组可以看作 一个神经层．每一层中的神经元接收前一层神经元的输出，并输出到下一层神经 元．整个网络中的信息是朝一个方向传播，没有反向的信息传播，可以用一个有 向无环路图表示．第0层称为输入层，最后一层称 为输出层，其他中间层称为隐藏层．

记忆网络

和前馈网络相比，记忆网络中的神经元具有记 忆功能，在不同的时刻具有不同的状态．记忆神经网络中的信息传播可以是单向 或双向传递，因此可用一个有向循环图或无向图来表示．

 图网络

图网络是定义在图结构数据上的神经网络（第6.8节）．图中每个节点都由 一个或一组神经元构成．节点之间的连接可以是有向的，也可以是无向的．每个 节点可以收到来自相邻节点或自身的信息．

![image-20230616111616961](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230616111616961.png)

 前馈神经网络

![image-20230616112837382](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230616112837382.png)

 应用到机器学习

多层前馈神经网络可以看作一个非线性复合函数 𝜙 ∶ ℝ𝐷 → ℝ𝐷′，将输入 𝒙 ∈ ℝ𝐷 映射到输出 𝜙(𝒙) ∈ ℝ𝐷′．因此，多层前馈神经网络也可以看成是一种特 征转换方法，其输出𝜙(𝒙)作为分类器的输入进行分类．

![image-20230616113133649](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230616113133649.png)

 反向传播算法

使用误差反向传播算法的前馈神经网络训练过程可以分为以下三步： 

（1） 前馈计算每一层的净输入𝒛 (𝑙) 和激活值𝒂 (𝑙)，直到最后一层； 

（2） 反向传播计算每一层的误差项𝛿 (𝑙)； 

（3） 计算每一层参数的偏导数，并更新参数．

自动梯度计算

主流的深度学习框架都包含了自动梯度计算的功能，即我们可以只考 虑网络结构并用代码实现，其梯度可以自动进行计算，无须人工干预，这样可以 大幅提高开发效率． 自动计算梯度的方法可以分为以下三类：数值微分、符号微分和自动微分．

数值微分

![image-20230616131455134](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230616131455134.png)

符号微分

符号微分（Symbolic Differentiation）是一种基于符号计算的自动求导方 法． 和符号计算相对应的 概念是数值计算，即将 数值代入数学表示中 进行计算． 符号计算也叫代数计算，是指用计算机来处理带有变量的数学表达式．这里 的变量被看作符号（Symbols），一般不需要代入具体的值．符号计算的输入和输 出都是数学表达式，一般包括对数学表达式的化简、因式分解、微分、积分、解代 数方程、求解常微分方程等运算．

自动微分

自动微分的基本原理是所有的数值计算可以分解为一些基本操作，包含 +, −, ×, / 和一些初等函数 exp, log,sin, cos 等，然后利用链式法则来自动计算一 个复合函数的梯度．

![image-20230616132513269](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230616132513269.png)

前向模式 前向模式是按计算图中计算方向的相同方向来递归地计算梯度.

反向模式 反向模式是按计算图中计算方向的相反方向来递归地计算梯度．

前向模式和反向模式可以看作应用链式法则的两种梯度累积方式．从反向模式的计算顺序可以看出，反向模式和反向传播的计算梯度的方式相同．

总结和深入阅读

![image-20230616133545334](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230616133545334.png)

卷积神经网络

  卷积神经网络最早主要是用来处理图像信息．在用全连接前馈网络来处理 图像时，会存在以下两个问题：

（1） 参数太多：如果输入图像大小为100 × 100 × 3（即图像高度为100，宽 度为 100 以及 RGB 3 个颜色通道），在全连接前馈网络中，第一个隐藏层的每个 神经元到输入层都有 100 × 100 × 3 = 30 000 个互相独立的连接，每个连接都对 应一个权重参数．随着隐藏层神经元数量的增多，参数的规模也会急剧增加．这 会导致整个神经网络的训练效率非常低，也很容易出现过拟合． （2） 局部不变性特征：自然图像中的物体都具有局部不变性特征，比如尺 度缩放、平移、旋转等操作不影响其语义信息．而全连接前馈网络很难提取这些 局部不变性特征，一般需要进行数据增强来提高性能．

目前的卷积神经网络一般是由卷积层、汇聚层和全连接层交叉堆叠而成的 前馈神经网络．

 一维卷积

一维卷积经常用在信号处理中，用于计算信号的延迟累积．

![image-20230617095009171](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230617095009171.png)

图5.1给出了两个滤波器的一维卷积示例．可以看出，两个滤波器分别提取 了输入序列的不同特征．滤波器 𝒘 = [1/3, 1/3, 1/3] 可以检测信号序列中的低频 信息，而滤波器𝒘 = [1, −2, 1]可以检测信号序列中的高频信息．

![image-20230617095101980](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230617095101980.png)

二维卷积

![image-20230617095345882](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230617095345882.png)

![image-20230617095414683](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230617095414683.png)

卷积的变种

步长（Stride）是指卷积核在滑动时的时间间隔．图5.4a给出了步长为 2 的 卷积示例． 步长也可以小于 1， 即微步卷积，参见 第5.5.1节． 

零填充（Zero Padding）是在输入向量两端进行补零．图5.4b给出了输入的 两端各补一个零后的卷积示例．

 用卷积来代替全连接

![image-20230617100434528](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230617100434528.png)

局部连接 在卷积层（假设是第𝑙 层）中的每一个神经元都只和下一层（第𝑙 − 1 层）中某个局部窗口内的神经元相连，构成一个局部连接网络．如图5.5b所示，卷 积层和下一层之间的连接数大大减少，由原来的 𝑀𝑙 × 𝑀𝑙−1 个连接变为 𝑀𝑙 × 𝐾 个连接，𝐾 为卷积核大小． 

权重共享 从公式(5.22)可以看出，作为参数的卷积核𝒘(𝑙) 对于第𝑙 层的所有的 神经元都是相同的．如图5.5b中，所有的同颜色连接上的权重是相同的．权重共享可以理解为一个卷积核只捕捉输入数据中的一种特定的局部特征．因此，如果 要提取多种特征就需要使用多个不同的卷积核．

![image-20230617100917691](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230617100917691.png)

卷积层

不失一般性，假设一个卷积层的结构如下： 

（1） 输入特征映射组：𝒳 ∈ ℝ𝑀×𝑁×𝐷 为三维张量（Tensor），其中每个切 片（Slice）矩阵𝑿 𝑑 ∈ ℝ𝑀×𝑁 为一个输入特征映射，1 ≤ 𝑑 ≤ 𝐷；

 （2） 输出特征映射组：𝒴 ∈ ℝ𝑀′×𝑁′×𝑃 为三维张量，其中每个切片矩阵 𝒀 𝑝 ∈ ℝ𝑀′×𝑁′ 为一个输出特征映射，1 ≤ 𝑝 ≤ 𝑃；

（3） 卷积核：𝒲 ∈ ℝ𝑈×𝑉×𝑃×𝐷 为四维张量，其中每个切片矩阵 𝑾𝑝,𝑑 ∈ ℝ𝑈×𝑉 为一个二维卷积核，1 ≤ 𝑝 ≤ 𝑃, 1 ≤ 𝑑 ≤ 𝐷．

![image-20230617102612190](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230617102612190.png)

汇聚层

汇聚层（Pooling Layer）也叫子采样层（Subsampling Layer），其作用是进 行特征选择，降低特征数量，从而减少参数数量．

卷积层虽然可以显著减少网络中连接的数量，但特征映射组中的神经元个 数并没有显著减少．如果后面接一个分类器，分类器的输入维数依然很高，很容 易出现过拟合．为了解决这个问题，可以在卷积层之后加上一个汇聚层，从而降 低特征维数，避免过拟合． 

（1） 最大汇聚（Maximum Pooling或Max Pooling）：对于一个区域𝑅 𝑑 𝑚,𝑛， 选择这个区域内所有神经元的最大活性值作为这个区域的表示。

（2） 平均汇聚（Mean Pooling）：一般是取区域内所有神经元活性值的平 均值。

![image-20230617103230798](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230617103230798.png)
