#### 1.完成了《PyTorch深度学习实践》的学习

（1）学习了Logistic Regression及其实例 

sigmoid函数：在神经网络中，sigmoid函数通常作为激活函数使用。它可以帮助神经网络学习非线性的特征，从而提高模型的拟合能力。另外，在二分类问题中，sigmoid函数也可以用于计算输出层的概率值，从而判断输入数据属于哪一类。

学习了Logistic Regression的计算模块和损失函数BCE,并和线性模型的MSE做比较。

```python

class LogisticRegressionModel(torch.nn.Module):
    def __init__(self):
        super(LogisticRegressionModel, self).__init__()
        self.linear = torch.nn.Linear(1,1)
 
    def forward(self, x):
        # y_pred = F.sigmoid(self.linear(x))
        y_pred = torch.sigmoid(self.linear(x))
        return y_pred
model = LogisticRegressionModel()
 
# construct loss and optimizer
# 默认情况下，loss会基于element平均，如果size_average=False的话，loss会被累加。
criterion = torch.nn.BCELoss(size_average = False) 
optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)
```

（2）学习了多维特征的输入

多维特征当作矩阵输入，修改linear的参数值。矩阵就是空间变换的函数，可以降维和升维。

```
xy = np.loadtxt('diabetes.csv', delimiter=',', dtype=np.float32)
x_data = torch.from_numpy(xy[:, :-1]) # 第一个‘：’是指读取所有行，第二个‘：’是指从第一列开始，最后一列不要
print("input data.shape", x_data.shape)
y_data = torch.from_numpy(xy[:, [-1]]) # [-1] 最后得到的是个矩阵
```

了解了  torch.sigmoid()、torch.nn.Sigmoid()、torch.nn.functional.sigmoid()三者之间的区别，功能是一样的，定义和使用不同。

（3）学习使用Dataloader和Dataset

1、DataSet 是抽象类，不能实例化对象，主要是用于构造我们的数据集

​      2、DataLoader 需要获取DataSet提供的索引[i]和len;用来帮助我们加载数据，比如说做shuffle(提高数据集的随机性)，batch_size,能拿出Mini-Batch进行训练。它帮我们自动完成这些工作。DataLoader可实例化对象。

（4）多分类问题

`criterion = torch.nn.CrossEntropyLoss()`既可以解决二分类问题，也可以解决多分类问题。

![image-20230504105423540](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230504105423540.png)

1.神经网络输出不需要再做非线性变化。

2.y必须是longtensor。

![image-20230504111715022](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230504111715022.png)

softmax的输入不需要再做非线性变换，也就是说softmax之前不再需要激活函数(relu)。softmax两个作用，如果在进行softmax前的input有负数，通过指数变换，得到正数。所有类的概率求和为1。

了解了transforms和view

transforms是将图像由读取进来的PIL转换为Tensor，并且转换为cwh通道，并将值归一化。

view把张量变成矩阵

（5）CNN

![image-20230507144351547](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230507144351547.png)

concatenate:把张量合在一起。

1*1张量作用：减少计算。

梯度消失

![image-20230507160652867](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230507160652867.png)

解决方法：Deep Residual Learning，使得梯度趋近于1而非0。

residual block

![image-20230507162421153](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230507162421153.png)

它的作用是解决深度神经网络中的梯度消失和梯度爆炸问题，从而提高模型的训练效果。提高泛化能力。

（6）RNN

![image-20230508134839263](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230508134839263.png)

1.每个RNN Cell都是同一线性层。

2.共享权重，减少权重使用量。

3.输入必须是longtensor。

![image-20230509163851627](C:\Users\lxc\AppData\Roaming\Typora\typora-user-images\image-20230509163851627.png)

2.重新配置了conda环境

解决了GPU不能使用的问题，还为pycharm增加了解释器。

3.了解了COLAB	

（1）Colab还支持Python、Markdown等多种编程语言和文本格式.

（2）免费的云端Jupyter笔记本环境，可以用于数据分析、机器学习、深度学习等任务。可以连接GitHub。

但是额度有限，并且不稳定。